{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxies-Classification-By-Using-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 서론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN(Convolution Neural Network)은 합성곱 계층(convolutional layer)과 풀링 계층(pooling layer), 그리고 완전연결 계층(fully-connected layer)로 구성되어 특히 이미지데이터처럼 가까운 데이터끼리 서로 연관성이 높을 때 좋은 성능을 내는 모델이다. 그 구조의 예는 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Ahe2Aoo.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP에서 사용하는 완전연결 계층은 이미지 데이터를 학습하기에 문제점을 가지고 있다. 데이터의 형상이 무시된다는 것이 그것인데, 가로, 세로, 채널(색상)로 구성된 3차원 데이터인 이미지를 평평한 1차원 데이터로 평탄화해주기 때문이다. 그 다음 모든 뉴런과 연결되어 각각의 weight와 bias를 가지고 연산하기 때문에 형상을 무시하고 모든 입력 데이터를 동등한 뉴런(같은 차원의 뉴런)으로 취급한다.<br><br>\n",
    "하지만 이미지는 분명 3차원 형상이며, 따라서 공간적 정보가 중요한 요소가 된다. 서로 인접한 픽셀끼리는 유사한 값을 가지고 있다던지, 반대로 서로 거리가 먼 픽셀끼리는 큰 관련성이 없다던지, RGB채널 각각은 서로 밀접한 관련이 있다는 등 수많은 공간적 정보가 존재한다. 따라서 완전연결 계층만으로는 해결할 수 없었던 문제를 해결해주는 방법이 CNN이다.<br><br>\n",
    "Convolution Layer는 합성곱 계층과 풀링 계층으로 이루어져 있다. 합성곱 계층은 채널별로 filter를 가지고 있는데, 이 filter를 이용해 feature map과 합성곱을 통해 output을 도출하고 backpropagation을 통해 filter의 weight들을 학습시킨다. 이 filter는 n by n의 정사각형 형태이기 때문에 합성곱을 할 때 주위 인접한 데이터들을 기준으로 하기 때문에 공간적 정보를 반영하여 학습을 할 수 있는 것이다. 또한 풀링 계층은 가로, 세로 방향의 공간을 줄이는 연산으로써 feature map의 중요한 정보는 최대한 보존한 채 output을 도출해낼 수 있다.<br><br>\n",
    "따라서 이번 프로젝트를 통해 CNN을 직접 구상해봄으로써 이미지 데이터를 모델에 학습 및 평가해보고 직면하는 문제점들을 해결해나가는 것을 중점으로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에 수행한 프로젝트는 '딥러닝으로 은하 분류하기'이다. 이는 데이터사이언스 데이터와 문제를 해결하기 위한 질문식 가이드를 제공해주는 사이트인 DAFIT에 게시되어있는 프로젝트이다. 프로젝트 관련 설명은 아래 링크를 통해 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 프로젝트 주소<br>\n",
    "http://www.dafit.me/question/?q=YToxOntzOjEyOiJrZXl3b3JkX3R5cGUiO3M6MzoiYWxsIjt9&bmode=view&idx=2547211&t=board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 연구"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 연구는 크게 3가지 범주로 나누어 수행했다. 첫 번째는 'Tutorial and Concept'으로 DAFIT에서 제공하는 튜토리얼을 따라 수행하며 CNN을 수행하기 위한 일련의 절차들을 배운다. 두 번째는 'Pratice_Modeling'으로 CNN 개념을 바탕으로 직접 모델을 설계하고 수정하며 최고의 성능을 도출하는 모델을 생성해낸다. 세 번째는 'Pratice_VGG16'으로 기존 Image 분야에서 높은 성능을 보였던 VGG16을 가져와서 사용하면 어떤 성능이 나올지 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tutorial and Concept](https://github.com/YoonSungLee/Galaxies-Classification-By-Using-Deep-Learning/blob/master/Tutorial_and_Concept.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc: 76.50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(50,50,3)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial에서 제공한 모델은 매우 단순한 CNN구조로써 Conv2D-Conv2D-MaxPooling2D의 Convolution layer와 Dense(128)-Dense(3)의 fully-connected layer로 이루어져있다. BatchNormalization, Dropout, EarlyStopping 등의 Overfitting을 억제하는 기법들을 사용하지 않고 모델을 학습한다면 76.5%의 정확도를 얻을 수 있다. 이는 4개의 이미지 중에 1개는 틀린다는 의미로 성능이 그닥 좋지 않은 모델이라고 할 수 있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pratice Modeling](https://github.com/YoonSungLee/Galaxies-Classification-By-Using-Deep-Learning/blob/master/Pratice_Modeling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy : 85.62%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AI Innovation Square에서 배운 CNN의 몇 가지 모델링 기법을 적용했다.\n",
    "* feature map이 홀수가 되는 것을 방지하기 위해 첫 번째 Convolution Layer는 1개만 쌓는 방식을 택했다.\n",
    "* 과적합을 방지하고 정규화를 위해 Convolution Layer 사이에 BatchNormalization을 적용했다.\n",
    "* 기존의 CNN에서 feature map을 flatten하는 방식이 비효율적이라고 판단하여 새롭게 등장한 방법인 GlobalAveragePoolin2D를 적용하여 마지막 Layer와 연결했다(이는 각 feature map의 평균을 구해서 flatten처럼 길게 배열).\n",
    "* 마지막 Layer에서 Dense Layer를 더 추가하지 않았다. 만약 추가한다면 Dense Layer 사이에 Dropout을 이용하여 과적합을 방지하는 전략을 써야 할 것이다.\n",
    "* Convolution Layer : activation='relu'(CNN에서 좋은 효과를 보이는 활성화함수 적용)\n",
    "* Convolution Layer : kernel_initializer='he_normal'(activation function을 relu로 설정했기 때문)\n",
    "* Dense Layer : activation='softmax'(다중 분류문제이기 때문)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_11 (Conv2D)           (None, 48, 48, 32)        896       \n",
    "_________________________________________________________________\n",
    "batch_normalization_7 (Batch (None, 48, 48, 32)        128       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_7 (MaxPooling2 (None, 24, 24, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_12 (Conv2D)           (None, 22, 22, 32)        9248      \n",
    "_________________________________________________________________\n",
    "conv2d_13 (Conv2D)           (None, 20, 20, 64)        18496     \n",
    "_________________________________________________________________\n",
    "batch_normalization_8 (Batch (None, 20, 20, 64)        256       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_8 (MaxPooling2 (None, 10, 10, 64)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_14 (Conv2D)           (None, 8, 8, 64)          36928     \n",
    "_________________________________________________________________\n",
    "conv2d_15 (Conv2D)           (None, 6, 6, 128)         73856     \n",
    "_________________________________________________________________\n",
    "batch_normalization_9 (Batch (None, 6, 6, 128)         512       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 128)         0         \n",
    "_________________________________________________________________\n",
    "global_average_pooling2d_3 ( (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 3)                 387       \n",
    "=================================================================\n",
    "Total params: 140,707\n",
    "Trainable params: 140,259\n",
    "Non-trainable params: 448\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/RyCdg1r.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* best_model-06-0.89.h5\n",
    "* 모델 학습곡선을 살펴보면 train data에 대해서는 꾸준한 학습을 보이는 데 반해, validation data에 대해서는 결과값들이 들쭉날쭉하고 좋은 성능을 보이지 못하고 있다.\n",
    "* fully-connected layer의 층을 두껍게 하고 Dropout을 추가하는 방법을 적용해볼 수 있다. 추가적으로 GlobalAveragePooling2D의 적용이 이 데이터셋에 대해서는 좋지 않은 성능을 보일 수 있으므로 단순히 Flatten을 적용하는 방법도 고려해볼 수 있다.\n",
    "* 학습 조기종료가 상대적으로 빨리 이루어진듯한 느낌이 있다. 이는 Overfitting이 너무 빨리 이루어졌거나 Local Minimum Problem일 가능성이 있다. 이를 좀 더 잘 판단하기 위해 EarlyStopping의 patience을 조정해줄 필요가 있다.\n",
    "* 위의 Feedback을 했는데도 큰 발전이 없다면 Convolution Layer의 구조를 바꿀 수 밖에 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy : 86.88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fully-connected layer에 Dense Layer와 Dropout 2개를 추가했다.\n",
    "* EarlyStopping의 patience을 20으로 늘려 학습 조기종료시간을 조금 더 늦췄다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_16 (Conv2D)           (None, 48, 48, 32)        896       \n",
    "_________________________________________________________________\n",
    "batch_normalization_10 (Batc (None, 48, 48, 32)        128       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_10 (MaxPooling (None, 24, 24, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_17 (Conv2D)           (None, 22, 22, 32)        9248      \n",
    "_________________________________________________________________\n",
    "conv2d_18 (Conv2D)           (None, 20, 20, 64)        18496     \n",
    "_________________________________________________________________\n",
    "batch_normalization_11 (Batc (None, 20, 20, 64)        256       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_11 (MaxPooling (None, 10, 10, 64)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_19 (Conv2D)           (None, 8, 8, 64)          36928     \n",
    "_________________________________________________________________\n",
    "conv2d_20 (Conv2D)           (None, 6, 6, 128)         73856     \n",
    "_________________________________________________________________\n",
    "batch_normalization_12 (Batc (None, 6, 6, 128)         512       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_12 (MaxPooling (None, 3, 3, 128)         0         \n",
    "_________________________________________________________________\n",
    "global_average_pooling2d_4 ( (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 128)               16512     \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 128)               16512     \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_6 (Dense)              (None, 3)                 387       \n",
    "=================================================================\n",
    "Total params: 173,731\n",
    "Trainable params: 173,283\n",
    "Non-trainable params: 448\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/GwG7SkQ.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* best_model2-09-0.87.h5\n",
    "* Model 1과 비교하여 아주 미세하게 튀는 현상이 줄긴 했지만 여전히 validation data에 대해서 불안정한 결과가 많이 보인다.\n",
    "* fully-connected layer의 층을 쌓은 효과를 보지 못했다. 다른 방법으로는 GlobalAveragePooling2D 대신에 단순히 Flatten을 적용하여 feature map을 1차원 데이터로 펼치는 방법이다. GlobalAveragePooling2D이 Flatten보다 이미지 데이터에 대하여 좋은 성능을 보인다고는 하지만 모델링 방법에 따라 그리고 이미지 데이터의 특성에 따라 얼마든지 예외인 경우가 있을 수 있다.\n",
    "* 또한 마찬가지로 Overfitting 또는 Local Minimum Problem 등의 문제로 적은 수의 epoch으로 학습 조기 중단이 발생했다. 파라미터를 미세하게 수정할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy : 85.00%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 48, 48, 32)        896       \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 22, 22, 32)        9248      \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 20, 20, 64)        18496     \n",
    "_________________________________________________________________\n",
    "batch_normalization_2 (Batch (None, 20, 20, 64)        256       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 64)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928     \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 6, 6, 128)         73856     \n",
    "_________________________________________________________________\n",
    "batch_normalization_3 (Batch (None, 6, 6, 128)         512       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_6 (Conv2D)            (None, 1, 1, 128)         147584    \n",
    "_________________________________________________________________\n",
    "batch_normalization_4 (Batch (None, 1, 1, 128)         512       \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 128)               16512     \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 128)               16512     \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 3)                 387       \n",
    "=================================================================\n",
    "Total params: 321,827\n",
    "Trainable params: 321,123\n",
    "Non-trainable params: 704\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qYlysiJ.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* best_model3-13-0.85.h5\n",
    "* Model 2와 비교하여 학습곡선이 더욱 불안정하고 val_loss와 val_acc 또한 더 좋지 않은 결과를 보이고 있다. 이로써 Flatten보다는 GlobalAveragePooling2D가 더 좋은 효과를 낸다는 것을 확인할 수 있었다.\n",
    "* fully-connected layer에서 문제가 없다면 결국 Convolution Layer를 손댈 수 밖에 없다. Image 분야에서 좋은 성능을 낸 모델들의 구성과 비슷하게 만들어보는 방법이 있고, 그냥 그 모델 그대로 가져와서 학습하는 방법이 있다. 모델링에 초점을 맞추어 Convolution Layer의 구성을 바꾸는 방법을 택하기로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy : 87.29%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한때 Image데이터에 대단한 성능을 보였던 VGG16 모델을 개조한 모델이다.\n",
    "* VGG16은 input data의 shape이 (224,224)로 고정되어있으므로 ImageDataGenerator 인스턴스들의 target_size를 알맞게 조정해준다.\n",
    "* 모든 layer의 activation function을 'relu'로, kernel initializer를 'he_normal'로 설정했다.\n",
    "* Convolution - Convolution - MaxPooling을 한 블록이라고 한다면, 블록과 블록 사이에 BatchNormalization을 적용하여 모델의 성능을 높이고자 했다.\n",
    "* 기존 VGG16의 bottleneck을 완화하기 위해 Maxpooling2D를 하기 전에 filter의 수를 미리 2배로 늘려주었다.\n",
    "* 기존 VGG16의 fully-connected layer의 Flatten 대신 GlobalAveragePooling2D를 사용하고, 이에 따라 Dense layer의 units 또한 적절히 수정했다.\n",
    "* fully-connected layer의 Dense layer 사이에 Dropout을 적용하여 Dense layer들의 Overfitting을 방지하고자 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 224, 224, 128)     73856     \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batch (None, 224, 224, 128)     512       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 128)     0         \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 112, 112, 256)     295168    \n",
    "_________________________________________________________________\n",
    "batch_normalization_2 (Batch (None, 112, 112, 256)     1024      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 256)       0         \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
    "_________________________________________________________________\n",
    "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
    "_________________________________________________________________\n",
    "conv2d_7 (Conv2D)            (None, 56, 56, 512)       1180160   \n",
    "_________________________________________________________________\n",
    "batch_normalization_3 (Batch (None, 56, 56, 512)       2048      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 512)       0         \n",
    "_________________________________________________________________\n",
    "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "batch_normalization_4 (Batch (None, 28, 28, 512)       2048      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
    "_________________________________________________________________\n",
    "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "batch_normalization_5 (Batch (None, 14, 14, 512)       2048      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
    "_________________________________________________________________\n",
    "global_average_pooling2d_1 ( (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 3)                 1539      \n",
    "=================================================================\n",
    "Total params: 17,572,099\n",
    "Trainable params: 17,568,259\n",
    "Non-trainable params: 3,840\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Yj6r5Iu.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* best_model4-25-0.91.h5\n",
    "* ModelCheckpoint를 통해 저장된 모델을 기준으로 했을 때 이전 모델들에 비해서 좋은 성능을 낸 것은 맞다. 하지만 학습곡선을 두고 봤을 땐 오히려 더 불안정한 경향이 있다. 쌓은 layer에 비해 큰 성능을 내지 못하고 있다.\n",
    "* AI Innovation Square에서 배운 내용을 바탕으로 한 가지 모델을 더 실험할 계획이다. 먼저 모든 Convolution가 통과된 이후 BatchNormalization을 적용시킨다. 또한 BatchNormalization의 파라미터가 Convolution Layer의 bias 역할을 대신 해주므로 모든 Convolution Layer의 bias를 제거시켜준다(use_bias=False).\n",
    "* 추가적으로 EarlyStopping이 너무 늦다고 판단하여 patience 조절이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy : 84.79%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model 4의 Convolution Layer의 구조를 Conv2D-BN-Conv2D-BN-MaxPooling2D 형식으로 바꾼다.\n",
    "* Convolution Layer의 bias를 제거시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_14 (Conv2D)           (None, 224, 224, 64)      1728      \n",
    "_________________________________________________________________\n",
    "batch_normalization_14 (Batc (None, 224, 224, 64)      256       \n",
    "_________________________________________________________________\n",
    "conv2d_15 (Conv2D)           (None, 224, 224, 128)     73728     \n",
    "_________________________________________________________________\n",
    "batch_normalization_15 (Batc (None, 224, 224, 128)     512       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 128)     0         \n",
    "_________________________________________________________________\n",
    "conv2d_16 (Conv2D)           (None, 112, 112, 128)     147456    \n",
    "_________________________________________________________________\n",
    "batch_normalization_16 (Batc (None, 112, 112, 128)     512       \n",
    "_________________________________________________________________\n",
    "conv2d_17 (Conv2D)           (None, 112, 112, 256)     294912    \n",
    "_________________________________________________________________\n",
    "batch_normalization_17 (Batc (None, 112, 112, 256)     1024      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 256)       0         \n",
    "_________________________________________________________________\n",
    "conv2d_18 (Conv2D)           (None, 56, 56, 256)       589824    \n",
    "_________________________________________________________________\n",
    "batch_normalization_18 (Batc (None, 56, 56, 256)       1024      \n",
    "_________________________________________________________________\n",
    "conv2d_19 (Conv2D)           (None, 56, 56, 256)       589824    \n",
    "_________________________________________________________________\n",
    "batch_normalization_19 (Batc (None, 56, 56, 256)       1024      \n",
    "_________________________________________________________________\n",
    "conv2d_20 (Conv2D)           (None, 56, 56, 512)       1179648   \n",
    "_________________________________________________________________\n",
    "batch_normalization_20 (Batc (None, 56, 56, 512)       2048      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 512)       0         \n",
    "_________________________________________________________________\n",
    "conv2d_21 (Conv2D)           (None, 28, 28, 512)       2359296   \n",
    "_________________________________________________________________\n",
    "batch_normalization_21 (Batc (None, 28, 28, 512)       2048      \n",
    "_________________________________________________________________\n",
    "conv2d_22 (Conv2D)           (None, 28, 28, 512)       2359296   \n",
    "_________________________________________________________________\n",
    "batch_normalization_22 (Batc (None, 28, 28, 512)       2048      \n",
    "_________________________________________________________________\n",
    "conv2d_23 (Conv2D)           (None, 28, 28, 512)       2359296   \n",
    "_________________________________________________________________\n",
    "batch_normalization_23 (Batc (None, 28, 28, 512)       2048      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 512)       0         \n",
    "_________________________________________________________________\n",
    "conv2d_24 (Conv2D)           (None, 14, 14, 512)       2359296   \n",
    "_________________________________________________________________\n",
    "batch_normalization_24 (Batc (None, 14, 14, 512)       2048      \n",
    "_________________________________________________________________\n",
    "conv2d_25 (Conv2D)           (None, 14, 14, 512)       2359296   \n",
    "_________________________________________________________________\n",
    "batch_normalization_25 (Batc (None, 14, 14, 512)       2048      \n",
    "_________________________________________________________________\n",
    "conv2d_26 (Conv2D)           (None, 14, 14, 512)       2359296   \n",
    "_________________________________________________________________\n",
    "batch_normalization_26 (Batc (None, 14, 14, 512)       2048      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_10 (MaxPooling (None, 7, 7, 512)         0         \n",
    "_________________________________________________________________\n",
    "global_average_pooling2d_2 ( (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "dropout_4 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_6 (Dense)              (None, 3)                 1539      \n",
    "=================================================================\n",
    "Total params: 17,578,435\n",
    "Trainable params: 17,569,091\n",
    "Non-trainable params: 9,344\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/0lHwbRR.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* best_model5-21-0.85.h5\n",
    "* 전체적인 val_loss가 조금씩 줄어든 양상이 보이긴 하나 그 차이가 미세하다. 그래도 효과는 어느 정도 있기 때문에 모든 Convolution Layer 다음에 BatchNormalization을 적용하는 것은 바람직한 방법이라 생각한다. 이에 따라 Convolution Layer의 bias를 사용하지 않도록 설정하는 것 또한 상기시켜야 할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Modeling Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test set의 정확도 결과는 위의 내용과 같다. 가장 낮은 정확도를 도출한 모델은 Model 5(84.79%)이고, 가장 높은 정확도를 도출한 모델은 Model 4(89.29%)이다. 이 결과를 놓고 보면 validation set을 통한 모델들의 기대성능과는 또다른 결과라고 할 수 있겠다. Model 4의 성능이 나쁘지 않아 BatchNormalization을 더 추가하고 Convolution Layer의 use_bias를 False로 설정하면 Model 5의 성능이 더 좋을 것이다라고 추측했는데, 실상은 오히려 가장 낮은 정확도를 보이기 때문이다. 이 원인에는 여러가지가 존재한다. validation set을 통한 평가결과가 운이 좋게 나온것일수도 있고, best_model의 기준을 accuracy가 아닌 loss로 설정한 탓일수도 있다. 또는 이번 test set에서만 유독 성능이 좋지 않을수도 있다. 하지만 중요한 점은 지금 내 관점으로는 정확한 원인 규명이 어렵고 모델의 성능을 더욱 좋게 할 수 있는 방도가 쉽게 떠오르지 않는다는 점이다. 이는 실무적인 면과 이론적인 면에서 모두 실력을 더 키워야 한다는 것을 반증한다.<br>\n",
    "추가적으로 이번 모델링을 할 때에는 하이퍼파라미터들에 대해 깊이 신경쓰지 않고 모델링에 중점을 두었다. 최적의 하이퍼파라미터 탐색도 하나의 중요한 이슈이므로 더욱 모델의 성능을 올리고 싶으면 이 또한 신경써줘야 하는 부분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pratice VGG16](https://github.com/YoonSungLee/Galaxies-Classification-By-Using-Deep-Learning/blob/master/Pratice_VGG16.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imagenet을 통해 이미 학습된 모델인 VGG16을 불러와서 가지고 있는 데이터를 학습하면 어떤 성능을 보일지 실험했다. 특히 크게 두 가지 전략으로 VGG16 모델을 사용했다. 첫 번째는 convolution layer의 모든 layer들이 재학습할 수 없도록(trainable=False) 설정했고, 두 번째는 convolution layer를 포함한 모든 layer들이 재학습할 수 있도록(trainable=True) 설정했다. 물론 fully-connected layer는 VGG16 모델이 아닌 Dense를 통해 4096, 2048, 1024, 3개의 units을 적용하여 3개의 클래스를 분류하는 문제에 적합하도록 설정했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16_Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 32.29%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pre-trained network\n",
    "* weight='imagenet'\n",
    "* vgg16_model1.trainable = False\n",
    "* Dense 4096, 2048, 1024, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
    "_________________________________________________________________\n",
    "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
    "_________________________________________________________________\n",
    "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
    "_________________________________________________________________\n",
    "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
    "_________________________________________________________________\n",
    "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
    "_________________________________________________________________\n",
    "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
    "_________________________________________________________________\n",
    "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
    "_________________________________________________________________\n",
    "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
    "_________________________________________________________________\n",
    "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
    "_________________________________________________________________\n",
    "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
    "_________________________________________________________________\n",
    "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
    "_________________________________________________________________\n",
    "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
    "_________________________________________________________________\n",
    "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
    "_________________________________________________________________\n",
    "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
    "_________________________________________________________________\n",
    "flatten_4 (Flatten)          (None, 25088)             0         \n",
    "_________________________________________________________________\n",
    "dense1 (Dense)               (None, 4096)              102764544 \n",
    "_________________________________________________________________\n",
    "dense2 (Dense)               (None, 2048)              8390656   \n",
    "_________________________________________________________________\n",
    "dense3 (Dense)               (None, 1024)              2098176   \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 3)                 3075      \n",
    "=================================================================\n",
    "Total params: 127,971,139\n",
    "Trainable params: 127,971,139\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/zI5vQoq.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16_Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 32.29%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pre-trained network\n",
    "* weight='imagenet'\n",
    "* vgg16_model1.trainable = True\n",
    "* Dense 4096, 2048, 1024, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
    "_________________________________________________________________\n",
    "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
    "_________________________________________________________________\n",
    "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
    "_________________________________________________________________\n",
    "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
    "_________________________________________________________________\n",
    "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
    "_________________________________________________________________\n",
    "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
    "_________________________________________________________________\n",
    "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
    "_________________________________________________________________\n",
    "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
    "_________________________________________________________________\n",
    "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
    "_________________________________________________________________\n",
    "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
    "_________________________________________________________________\n",
    "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
    "_________________________________________________________________\n",
    "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
    "_________________________________________________________________\n",
    "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
    "_________________________________________________________________\n",
    "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
    "_________________________________________________________________\n",
    "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 25088)             0         \n",
    "_________________________________________________________________\n",
    "dense1 (Dense)               (None, 4096)              102764544 \n",
    "_________________________________________________________________\n",
    "dense2 (Dense)               (None, 2048)              8390656   \n",
    "_________________________________________________________________\n",
    "dense3 (Dense)               (None, 1024)              2098176   \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 3)                 3075      \n",
    "=================================================================\n",
    "Total params: 127,971,139\n",
    "Trainable params: 127,971,139\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/L4ZNnyL.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pratice VGG16 Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기대와는 달리 결과는 처참했다. 복잡한 구조와 수많은 파라미터, 그리고 imagenet에 대해 대단한 성능을 보이던 VGG16의 성능은 두 가지 방법 모두 32.29%의 정확도를 보였다. 이는 Tutorial에서 제공하는 모델보다도 수준히 현저히 떨어지는 결과이다. 이에 대해서는 많은 이유가 있겠다. 첫 번째로 VGG16의 input layer의 input shape과 가지고 있는 train data shape의 불일치성이다. VGG16은 input shape이 (224,224,3)의 이미지를 대상으로 구성된 모델인 반면, 학습시켜야 할 이미지는 (50,50,3)의 이미지이다. VGG16의 input shape을 바꾸면 전체 모델이 깨지고 hidden layer 중에 dimension이 음수값이 나오는 상황이 발생한다. 따라서 ImageDataGenerator를 통해 이미지를 불러올 때 (224,224,3)의 shape으로 변형하는 방법을 사용했다. 아마도 이 변형과정에서 기존 이미지의 형태가 깨지고 분류하기 어려운 형태로 바뀔 가능성이 높다. 따라서 이러한 경우 주어진 이미지 데이터의 shape에 맞게 VGG16의 모든 layer의 구조를 조금씩 수정해 줄 필요가 있다. 두 번째로 주어진 데이터에 적합하지 않은 VGG16의 모델링 방식이다. VGG16 모델은 imagenet이라는 데이터에 최적화된 모델로써 모델링 시에 이 부분을 간과하지는 않았을 것이다. 아무리 잘 짜여진 모델이라고 해도 주어진 데이터가 어떤 것이냐에 따라서 성능이 바뀔 수 있는 것처럼, VGG16의 구조가 주어진 데이터에는 부적합하게 설계되었을 가능성이 있다는 것이다. 물론 내 경우엔 첫 번째 이유가 더 타당하다고 생각한다. 결국 이러한 이유들 또는 내가 알지 못하는 이유들때문에 좋은 성능을 보이지 못했고, 이번 경험을 바탕으로 차후 이미 학습된 모델을 불러와 사용할 경우 이러한 사항들을 명심해야 할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|model|accuracy|\n",
    "|:---:|---:|\n",
    "|Tutorial Model|76.50%|\n",
    "|Pratice Model 1|86.88%|\n",
    "|Pratice Model 2|85.62%|\n",
    "|Pratice Model 3|85.00%|\n",
    "|Pratice Model 4|87.29%|\n",
    "|Pratice Model 5|84.79%|\n",
    "|VGG16 Model 1|32.29%|\n",
    "|VGG16 Model 2|32.29%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 고찰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난번 MLP모델에 이어 이번에는 처음으로 CNN 모델을 구축해보았다. 결과적으로 Tutorial에서 제공한 모델(76.50%)보다 직접 설계한 모델(Pratice Model 4: 87.29%)을 통해 10.79% 정도 성능을 향상시켰다. 앞으로 더 나아가야 할 방향에 대해서 간략히 정리해두겠다.<br><br>\n",
    "\n",
    "**이미지 업로드 및 학습 시간의 문제**<br>\n",
    "Vision 분야를 공부하기 위해 피할 수 없는 문제에 직면했다. AWS나 Local GPU를 사용하지 않고 Google Colab을 통해 모델을 학습하던 방식에 한계가 온 것이다. Colab을 사용하기 위해 Image를 Google Drive에 업로드시켜야 하는데 그 시간이 대략 3시간 정도 걸렸고, 모델 하나를 구성해서 ImageDataGenerator를 통해 학습하는 데 또한 마찬가지로 굉장히 긴 시간이 걸렸다. 만약 VIsion 분야를 계속해서 공부할것이라면 이에 대한 해결책이 필수적이다.<br><br>\n",
    "\n",
    "**단순한 CNN 구조를 넘어서 기술적인 모델링 기법 습득 필요**<br>\n",
    "지금까지 구성한 모델은 Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dense, Dropout, Flatten 등의 단순한 CNN 구조의 조합으로 구성했다. 하지만 이러한 기법들로만 이용해서 고성능의 이미지 분류 모델을 구성하기엔 한계가 따르기 마련이다. 최근에 AI Innovation Square를 통해 DenseNet, GoogleNet, ResNet, U-Net, M-Net 등 Vision 분야에서 적용되는 많은 모델들과 그 기법들을 배우는 중인데, 이러한 기술적인 모델링 기법이 필요함을 느꼈다. 또한 이러한 기법을 잘 사용하려면 그 기법이 사용된 모델에 대한 충분한 이해가 뒷받침되어야 할 것이다. 따라서 Vision 분야에서 대표적인 모델들, 그리고 최신 모델들에 관한 논문을 읽고 이해하며, 구현할 필요성을 느꼈다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
